# -*- coding: utf-8 -*-
"""modelingattack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yZID-8APAEyT-bMtlQidGz-Wc44osPlw
"""

# modeling_attack_test.py
# Evaluates resistance to ML modeling attacks on PUF CRPs
# pip install pandas numpy scikit-learn

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("/content/data 1.csv")

# Convert challenge to binary vector (16 bits)
def hex_to_bin(hx):
    try:
        val = int(str(hx).replace('0x',''), 16)
        return np.array(list(map(int, format(val, '016b'))))
    except:
        return np.zeros(16, dtype=int)

X = np.vstack(df['challenge'].apply(hex_to_bin).values)
# Convert spike16 to response bit (you can use majresp or rx_hex if available)
y = np.array(df['spike16'] % 2, dtype=int)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression
lr = LogisticRegression(max_iter=500)
lr.fit(X_train, y_train)
lr_acc = accuracy_score(y_test, lr.predict(X_test))

# Support Vector Machine
svm = SVC(kernel='rbf')
svm.fit(X_train, y_train)
svm_acc = accuracy_score(y_test, svm.predict(X_test))

# Neural Network
mlp = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=500)
mlp.fit(X_train, y_train)
mlp_acc = accuracy_score(y_test, mlp.predict(X_test))

print(f"Logistic Regression Accuracy: {lr_acc:.3f}")
print(f"SVM Accuracy: {svm_acc:.3f}")
print(f"Neural Network Accuracy: {mlp_acc:.3f}")

# plot_modeling_attack_results.py
# Visualizes ML modeling resistance results

import matplotlib.pyplot as plt

# Your measured accuracies
models = ['Logistic Regression', 'SVM', 'Neural Network']
accuracies = [0.479, 0.492, 0.504]

# Ideal "random guessing" line (50%)
random_line = 0.5

# Plot
plt.figure(figsize=(6,4))
bars = plt.bar(models, accuracies, color=['#4C72B0','#55A868','#C44E52'], edgecolor='black')

# Random baseline
plt.axhline(random_line, color='gray', linestyle='--', linewidth=1.2, label='Random Guess (0.5)')

# Annotate values
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, acc + 0.005, f"{acc*100:.1f}%",
             ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.ylim(0.45, 0.55)
plt.ylabel('Prediction Accuracy')
plt.title('Modeling Attack Resistance of Bio–SNN–PUF')
plt.legend()
plt.tight_layout()
plt.show()

# deep_learning_attack_test.py
# pip install tensorflow pandas numpy scikit-learn

import tensorflow as tf
from tensorflow.keras import layers, models
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv("/content/data 1.csv")

# Convert challenge to 16-bit binary vector
def hex_to_bin(hx):
    try:
        val = int(str(hx).replace('0x',''), 16)
        return np.array(list(map(int, format(val, '016b'))))
    except:
        return np.zeros(16, dtype=int)

X = np.vstack(df['challenge'].apply(hex_to_bin).values)
y = np.array(df['spike16'] % 2, dtype=int)   # binary response

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build Deep Neural Network (3 hidden layers)
model = models.Sequential([
    layers.Input(shape=(16,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)

# Evaluate
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Deep Neural Network Accuracy: {acc:.3f}")

# plot_deep_learning_attack_results.py
# Visualize ML and Deep Learning resistance

import matplotlib.pyplot as plt

# Model accuracies
models = ['Logistic Regression', 'SVM', 'Neural Network', 'Deep Neural Network']
accuracies = [0.479, 0.492, 0.504, 0.516]

# Plot settings
plt.figure(figsize=(7,4))
bars = plt.bar(models, accuracies, color=['#4C72B0','#55A868','#C44E52','#8172B3'], edgecolor='black')

# Add random-guess line (50%)
plt.axhline(0.5, color='gray', linestyle='--', linewidth=1.2, label='Random Guess (0.5)')

# Annotate bars
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, acc + 0.005, f"{acc*100:.1f}%",
             ha='center', va='bottom', fontsize=10, fontweight='bold')

# Styling
plt.ylim(0.46, 0.55)
plt.ylabel('Prediction Accuracy')
plt.title('Modeling and Deep Learning Attack Resistance of Bio–SNN–PUF')
plt.legend()
plt.tight_layout()
plt.show()

# advanced_dl_attack.py
# Advanced deep-learning modeling attack (1D-CNN + Residual Blocks) for PUF CRPs
# Requirements: pip install tensorflow pandas numpy scikit-learn matplotlib

import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

DATA_CSV = "/content/data 1.csv"   # update path if needed

# ---------- Helpers ----------
def hex_to_bin_vec(hx, width=16):
    """Convert hex-like string to binary vector length `width` (MSB first)."""
    try:
        s = str(hx).strip()
        if s.startswith("0x") or s.startswith("0X"):
            s = s[2:]
        val = int(s, 16)
    except Exception:
        # fallback numeric
        try:
            val = int(float(s))
        except Exception:
            val = 0
    val &= (1 << width) - 1
    b = format(val, f'0{width}b')
    return np.array([int(ch) for ch in b], dtype=np.float32)

def load_dataset(csv_path):
    df = pd.read_csv(csv_path)
    if 'challenge' not in df.columns:
        raise ValueError("CSV must contain a 'challenge' column")
    # choose response bit: spike16 LSB (or use majresp if available)
    if 'spike16' in df.columns:
        responses = (df['spike16'].fillna(0).astype(int) & 1).values
    elif 'majresp' in df.columns:
        responses = df['majresp'].fillna(0).astype(int).values
    else:
        # try rx_hex LSB
        if 'rx_hex' in df.columns:
            responses = []
            for r in df['rx_hex']:
                try:
                    v = int(str(r), 16)
                except:
                    v = 0
                responses.append(v & 1)
            responses = np.array(responses, dtype=int)
        else:
            raise ValueError("No known response column found (spike16/majresp/rx_hex)")

    X = np.vstack(df['challenge'].apply(lambda h: hex_to_bin_vec(h, width=16)).values)
    y = responses.astype(np.int32)
    return X, y

# ---------- Model building ----------
def residual_block(x, filters, kernel_size=3, stride=1, dropout_rate=0.1):
    shortcut = x
    x = layers.Conv1D(filters, kernel_size, padding='same', strides=stride, kernel_regularizer=regularizers.l2(1e-5))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout_rate)(x)
    x = layers.Conv1D(filters, kernel_size, padding='same', strides=1, kernel_regularizer=regularizers.l2(1e-5))(x)
    x = layers.BatchNormalization()(x)
    # adjust shortcut if needed
    if shortcut.shape[-1] != x.shape[-1]:
        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)
        shortcut = layers.BatchNormalization()(shortcut)
    x = layers.Add()([x, shortcut])
    x = layers.Activation('relu')(x)
    return x

def build_advanced_model(input_length=16):
    inp = layers.Input(shape=(input_length, 1), name='challenge_input')

    # initial conv
    x = layers.Conv1D(32, kernel_size=3, padding='same', activation='relu')(inp)
    x = layers.BatchNormalization()(x)

    # stack of residual blocks (increasing filters)
    x = residual_block(x, filters=32, kernel_size=3, dropout_rate=0.15)
    x = residual_block(x, filters=64, kernel_size=3, dropout_rate=0.15)
    x = residual_block(x, filters=64, kernel_size=3, dropout_rate=0.15)

    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(0.3)(x)

    out = layers.Dense(1, activation='sigmoid', name='response')(x)

    model = models.Model(inputs=inp, outputs=out, name='Advanced1DResNet')
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

# ---------- Training & evaluation ----------
def train_and_evaluate(X, y, epochs=60, batch_size=64):
    # reshape for Conv1D: (N, 16, 1)
    X = X.astype(np.float32).reshape((-1, X.shape[1], 1))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)

    model = build_advanced_model(input_length=X.shape[1])
    model.summary()

    # callbacks
    es = callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True, verbose=1)
    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

    history = model.fit(X_train, y_train, validation_split=0.1, epochs=epochs,
                        batch_size=batch_size, callbacks=[es, rl], verbose=2)

    # Evaluate
    loss, acc = model.evaluate(X_test, y_test, verbose=0)
    y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()
    acc_score = accuracy_score(y_test, y_pred)

    print(f"\nTest Accuracy: {acc:.4f}, sklearn accuracy_score: {acc_score:.4f}")
    return model, history, (X_test, y_test, y_pred)

if __name__ == "__main__":
    print("Loading dataset...")
    X, y = load_dataset(DATA_CSV)
    print("Dataset shapes:", X.shape, y.shape)
    model, history, test_data = train_and_evaluate(X, y, epochs=60, batch_size=64)

# plot_deep_learning_resistance.py
# pip install matplotlib

import matplotlib.pyplot as plt

# Deep learning model names and accuracies
dl_models = ['Deep Neural Network (DNN)', 'Advanced DL (CNN–ResNet)']
accuracies = [0.502, 0.5175]  # Your measured accuracies

# Random-guess baseline
baseline = 0.5

# Create the bar plot
plt.figure(figsize=(6,4))
bars = plt.bar(dl_models, accuracies, color=['#8172B3', '#55A868'], edgecolor='black', width=0.5)

# Add random-guess line
plt.axhline(baseline, color='gray', linestyle='--', linewidth=1.2, label='Random Guess (0.5)')

# Annotate bar tops with percentage
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, acc + 0.003, f"{acc*100:.2f}%",
             ha='center', va='bottom', fontsize=10, fontweight='bold')

# Format and labels
plt.ylim(0.48, 0.54)
plt.ylabel('Prediction Accuracy')
plt.title('Resistance to Deep Learning Attacks of Bio–SNN–PUF')
plt.legend()
plt.tight_layout()
plt.show()

# cloning_resistance_test.py
# pip install pandas numpy matplotlib

import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt

def load_bits(csv):
    df = pd.read_csv(csv)
    return np.array([list(map(int, format(int(v)%65536, '016b')))
                     for v in df['spike16'].values])

device_files = ['device1.csv', 'device2.csv', 'device3.csv']  # add more if available
responses = [load_bits(f) for f in device_files]

# Compute pairwise inter-device Hamming distances
hd_values = []
for (i, A), (j, B) in itertools.combinations(enumerate(responses), 2):
    hd = np.mean(np.sum(A != B, axis=1) / 16)   # normalized HD per challenge
    hd_values.append(hd)
    print(f"Inter-device HD (Device {i+1} vs {j+1}): {hd:.3f}")

# Plot distribution
plt.hist(hd_values, bins=10, color='#4C72B0', edgecolor='black')
plt.axvline(0.5, color='gray', linestyle='--', label='Ideal (0.5)')
plt.xlabel('Normalized Inter-Device Hamming Distance')
plt.ylabel('Frequency')
plt.title('Cloning / Counterfeiting Resistance of Bio–SNN–PUF')
plt.legend()
plt.tight_layout()
plt.show()

print(f"Average Inter-device HD: {np.mean(hd_values):.3f}")